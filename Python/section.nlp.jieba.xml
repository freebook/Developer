<?xml version="1.0" encoding="UTF-8"?>
<section id="jieba">
	<title>结巴中文分词</title>
	<para>
		<ulink url="https://github.com/fxsjy/jieba" />
	</para>
	<para>安装</para>
	<screen>
		<![CDATA[
pip install jieba
pip install paddlepaddle
		]]>
	</screen>
	<section>
		<title>分词演示</title>
		<programlisting>
		<![CDATA[
# encoding=utf-8
import jieba
import paddle
paddle.enable_static()
jieba.enable_paddle()  # 启动paddle模式。 
strs = ["我来到北京清华大学", "乒乓球拍卖完了", "中国科学技术大学"]
for str in strs:
    seg_list = jieba.cut(str, use_paddle=True)  # 使用paddle模式
    print("Paddle Mode: " + '/'.join(list(seg_list)))

seg_list = jieba.cut("我来到北京清华大学", cut_all=True)
print("Full Mode: " + "/ ".join(seg_list))  # 全模式

seg_list = jieba.cut("我来到北京清华大学", cut_all=False)
print("Default Mode: " + "/ ".join(seg_list))  # 精确模式

seg_list = jieba.cut("他来到了网易杭研大厦")  # 默认是精确模式
print(", ".join(seg_list))

seg_list = jieba.cut_for_search("小明硕士毕业于中国科学院计算所，后在日本京都大学深造")  # 搜索引擎模式
print(", ".join(seg_list))
		]]>
		</programlisting>
	</section>
	<section>
		<title>日志设置</title>
		<programlisting>
		<![CDATA[
import jieba
import logging
logger = logging.getLogger()
# 配置 logger 禁止输出无用的信息
jieba.default_logger = logger

text = "他来到了网易杭研大厦"

words = jieba.cut(text)
print(", ".join(words))
print("-" * 50)
# 将 “杭研大厦”，“他来到了” 词频优先
jieba.suggest_freq('杭研大厦', True)
jieba.suggest_freq('他来到了', True)
words = jieba.cut(text)
print(", ".join(words))
		]]>
		</programlisting>
	</section>
	<section>
		<title>返回 generator</title>
		<para>默认 cut 返回 generator</para>
		<programlisting>
		<![CDATA[
# encoding=utf-8
import jieba
import paddle

segs = jieba.cut("转载请与作者联系，同时请务必标明文章原始出处和作者信息及本声明。")
print(type(segs))
print(", ".join(segs))		
		]]>
		</programlisting>
	</section>
	<section>
		<title>返回 list</title>
		<programlisting>
		<![CDATA[
# encoding=utf-8
import jieba
import paddle

segs = jieba.lcut("转载请与作者联系，同时请务必标明文章原始出处和作者信息及本声明。")
print(type(segs))
print(", ".join(segs))		
		]]>
		</programlisting>
	</section>
	<section>
		<title>精准模式与全模式比较</title>
		<programlisting>
		<![CDATA[
# encoding=utf-8
import jieba
import paddle
text = "转载请与作者联系，同时请务必标明文章原始出处和作者信息及本声明。"
segs = jieba.cut(text)  # 精准模式
print(", ".join(segs))
print("=" * 50)
segs = jieba.cut(text, cut_all=True)  # 全模式
print(", ".join(segs))
		]]>
		</programlisting>
		<para>输出结果</para>
		<screen>
		<![CDATA[
neo@MacBook-Pro-Neo ~/workspace/python/jieba % python3.9 /Users/neo/workspace/python/jieba/lcut.py
Building prefix dict from the default dictionary ...
Loading model from cache /var/folders/2f/jfnljdpn1t1dj_f61z2s8bwm0000gn/T/jieba.cache
Loading model cost 0.787 seconds.
Prefix dict has been built successfully.
转载, 请, 与, 作者, 联系, ，, 同时, 请, 务必, 标明, 文章, 原始, 出处, 和, 作者, 信息, 及本, 声明, 。
==================================================
转载, 请, 与, 作者, 联系, ，, 同时, 请, 务必, 标明, 明文, 文章, 原始, 出处, 和, 作者, 信息, 及, 本, 声明, 。		
		]]>
		</screen>
	</section>
	<section>
		<title>精准模式与搜索引擎模式比较</title>
		<programlisting>
		<![CDATA[
# encoding=utf-8
import jieba
import paddle
text = "小明硕士毕业于中国科学院计算所，后在日本京都大学深造"
segs = jieba.cut(text)  # 精准模式
print(", ".join(segs))
print("=" * 50)
searchs = jieba.cut_for_search(text)  # 搜索引擎模式
print(", ".join(searchs))
		]]>
		</programlisting>
		<para>输出结果</para>
		<screen>
		<![CDATA[
neo@MacBook-Pro-Neo ~/workspace/python/jieba % python3.9 /Users/neo/workspace/python/jieba/search.py
Building prefix dict from the default dictionary ...
Loading model from cache /var/folders/2f/jfnljdpn1t1dj_f61z2s8bwm0000gn/T/jieba.cache
Loading model cost 0.807 seconds.
Prefix dict has been built successfully.
小明, 硕士, 毕业, 于, 中国科学院, 计算所, ，, 后, 在, 日本京都大学, 深造
==================================================
小明, 硕士, 毕业, 于, 中国, 科学, 学院, 科学院, 中国科学院, 计算, 计算所, ，, 后, 在, 日本, 京都, 大学, 日本京都大学, 深造	
		]]>
		</screen>
	</section>
	<section>
		<title>词性标注</title>
		<screen>
		<![CDATA[
标签	含义		标签	含义		标签	含义		标签	含义
n	普通名词	f	方位名词	s	处所名词	t	时间
nr	人名		ns	地名		nt	机构名	nw	作品名
nz	其他专名	v	普通动词	vd	动副词	vn	名动词
a	形容词	ad	副形词	an	名形词	d	副词
m	数量词	q	量词		r	代词		p	介词
c	连词		u	助词		xc	其他虚词	w	标点符号
PER	人名		LOC	地名		ORG	机构名	TIME	时间
		
		]]>
		</screen>
		<para></para>
		<programlisting>
		<![CDATA[
import jieba
import jieba.posseg as pseg
import paddle
words = pseg.cut("我爱北京天安门")  # jieba默认模式
for word, flag in words:
    print('%s %s' % (word, flag))

print("="*40)

paddle.enable_static()
jieba.enable_paddle()  # 启动paddle模式。
words = pseg.cut("我爱北京天安门", use_paddle=True)  # paddle模式
for word, flag in words:
    print('%s %s' % (word, flag))
		
		]]>
		</programlisting>
		<para></para>
		<screen>
		<![CDATA[
neo@MacBook-Pro-Neo ~/workspace/python % python3.9 /Users/neo/workspace/python/jieba/seg.py
Building prefix dict from the default dictionary ...
Loading model from cache /var/folders/2f/jfnljdpn1t1dj_f61z2s8bwm0000gn/T/jieba.cache
Loading model cost 0.753 seconds.
Prefix dict has been built successfully.
我 r
爱 v
北京 ns
天安门 ns
========================================
Paddle enabled successfully......
我 r
爱 v
北京 LOC
天安门 LOC		
		]]>
		</screen>
	</section>
	<section>
		<title>词典管理</title>
		<section>
			<title>添加/删除词语</title>

			<programlisting>
		<![CDATA[
import jieba

text = "他来到了网易杭研大厦"

words = jieba.cut(text, HMM=False)
print(", ".join(words))
print("-" * 50)
jieba.add_word('杭研大厦')  # 将 “杭研大厦” 添加到词典
jieba.add_word('来到了')  # 将 “来到了” 添加到词典
words = jieba.cut(text, HMM=False)
print(", ".join(words))
print("-" * 50)
jieba.del_word('深圳')  # 将 “深圳” 从词典中删除
words = jieba.cut("我爱深圳", HMM=False)
print(", ".join(words))
		]]>
			</programlisting>
		</section>
		<section>
			<title>用户词典</title>
			<para>自定义词典</para>
			<programlisting>
			<![CDATA[
import jieba

jieba.load_userdict('dict.txt') # 载入用户词典
seg_list = jieba.cut("他来到了网易杭研大厦", HMM=False)  
print(", ".join(seg_list))

			]]>
			</programlisting>
		</section>
		<section>
			<title>自定义词库</title>
			<para>分词系统默认使用自带的词库，load_userdict 是在默认词库的基础上做加法操作。</para>
			<para>set_dictionary 是设置默认基础词库。</para>
			<programlisting>
			<![CDATA[
# encoding=utf-8
import jieba
jieba.set_dictionary('dict.txt')
seg_list = jieba.cut("转载请与作者联系，同时请务必标明文章原始出处和作者信息及本声明。")
print(seg_list)
print(", ".join(seg_list))			
			]]>
			</programlisting>
			<para>dict.txt</para>
			<programlisting>
			<![CDATA[
请务必 12 n
作者信息 10 n			
			]]>
			</programlisting>
		</section>
	</section>
	<section>
		<title>抽取文本标签</title>
		<section>
			<title>提取标签</title>
			<screen>
		<![CDATA[
方法参数：
jieba.analyse.extract_tags(sentence, topK=5, withWeight=True, allowPOS=())
参数说明：
sentence 需要提取的字符串，必须是str类型，不能是list
topK 提取前多少个关键字
withWeight 是否返回每个关键词的权重
allowPOS是允许的提取的词性，默认为allowPOS=‘ns’, ‘n’, ‘vn’, ‘v’，提取地名、名词、动名词、动词	
		]]>
			</screen>
			<programlisting>
			<![CDATA[
file = open('article.txt', 'r', encoding='utf-8')
contents = file.read()
print(jieba.analyse.extract_tags(sentence=contents, topK=20, allowPOS=('ns', 'n')))			
			]]>
			</programlisting>
		</section>
		<section>
			<title>基于 TextRank 算法的关键词抽取</title>
			<programlisting>
			<![CDATA[
import jieba.analyse
import jieba
import os

os.chdir('jieba')
file = open('article.txt', 'r', encoding='utf-8')
contents = file.read()
print(jieba.analyse.textrank(sentence=contents, topK=20, allowPOS=('ns', 'n')))			
			]]>
			</programlisting>
		</section>
	</section>
	<section>
		<title>返回词语在原文的起止位置</title>
		<programlisting>
		<![CDATA[
import jieba
import logging
logger = logging.getLogger()
jieba.default_logger = logger
text = u'大和服装饰品有限公司'
result = jieba.tokenize(text)
for tk in result:
    print("word %s\t\t start: %d \t\t end:%d" % (tk[0], tk[1], tk[2]))
print("-" * 50)
result = jieba.tokenize(text, mode='search')  # 搜索模式
for tk in result:
    print("word %s\t\t start: %d \t\t end:%d" % (tk[0], tk[1], tk[2]))
		
		]]>
		</programlisting>
		<para></para>
		<screen>
		<![CDATA[
		
		]]>
		</screen>
	</section>
</section>
