<?xml version="1.0" encoding="UTF-8"?>
<section id="jieba">
	<title>结巴中文分词</title>
	<para>
		<ulink url="https://github.com/fxsjy/jieba" />
	</para>
	<para>安装</para>
	<screen>
		<![CDATA[
pip install jieba
pip install paddlepaddle
		]]>
	</screen>
	<section>
		<title>分词演示</title>
		<programlisting>
		<![CDATA[
# encoding=utf-8
import jieba
import paddle
paddle.enable_static()
jieba.enable_paddle()  # 启动paddle模式。 
strs = ["我来到北京清华大学", "乒乓球拍卖完了", "中国科学技术大学"]
for str in strs:
    seg_list = jieba.cut(str, use_paddle=True)  # 使用paddle模式
    print("Paddle Mode: " + '/'.join(list(seg_list)))

seg_list = jieba.cut("我来到北京清华大学", cut_all=True)
print("Full Mode: " + "/ ".join(seg_list))  # 全模式

seg_list = jieba.cut("我来到北京清华大学", cut_all=False)
print("Default Mode: " + "/ ".join(seg_list))  # 精确模式

seg_list = jieba.cut("他来到了网易杭研大厦")  # 默认是精确模式
print(", ".join(seg_list))

seg_list = jieba.cut_for_search("小明硕士毕业于中国科学院计算所，后在日本京都大学深造")  # 搜索引擎模式
print(", ".join(seg_list))
		]]>
		</programlisting>
	</section>
	<section>
		<title>日志设置</title>
		<programlisting>
		<![CDATA[
import jieba
import logging
logger = logging.getLogger()
# 配置 logger 禁止输出无用的信息
jieba.default_logger = logger

text = "他来到了网易杭研大厦"

words = jieba.cut(text)
print(", ".join(words))
print("-" * 50)
# 将 “杭研大厦”，“他来到了” 词频优先
jieba.suggest_freq('杭研大厦', True)
jieba.suggest_freq('他来到了', True)
words = jieba.cut(text)
print(", ".join(words))
		]]>
		</programlisting>
	</section>
	<section>
		<title>返回 generator</title>
		<para>默认 cut 返回 generator</para>
		<programlisting>
		<![CDATA[
# encoding=utf-8
import jieba
import paddle

segs = jieba.cut("转载请与作者联系，同时请务必标明文章原始出处和作者信息及本声明。")
print(type(segs))
print(", ".join(segs))		
		]]>
		</programlisting>
	</section>
	<section>
		<title>返回 list</title>
		<programlisting>
		<![CDATA[
# encoding=utf-8
import jieba
import paddle

segs = jieba.lcut("转载请与作者联系，同时请务必标明文章原始出处和作者信息及本声明。")
print(type(segs))
print(", ".join(segs))		
		]]>
		</programlisting>
	</section>
	<section>
		<title>精准模式与全模式比较</title>
		<programlisting>
		<![CDATA[
# encoding=utf-8
import jieba
import paddle
text = "转载请与作者联系，同时请务必标明文章原始出处和作者信息及本声明。"
segs = jieba.cut(text)  # 精准模式
print(", ".join(segs))
print("=" * 50)
segs = jieba.cut(text, cut_all=True)  # 全模式
print(", ".join(segs))
		]]>
		</programlisting>
		<para>输出结果</para>
		<screen>
		<![CDATA[
neo@MacBook-Pro-Neo ~/workspace/python/jieba % python3.9 /Users/neo/workspace/python/jieba/lcut.py
Building prefix dict from the default dictionary ...
Loading model from cache /var/folders/2f/jfnljdpn1t1dj_f61z2s8bwm0000gn/T/jieba.cache
Loading model cost 0.787 seconds.
Prefix dict has been built successfully.
转载, 请, 与, 作者, 联系, ，, 同时, 请, 务必, 标明, 文章, 原始, 出处, 和, 作者, 信息, 及本, 声明, 。
==================================================
转载, 请, 与, 作者, 联系, ，, 同时, 请, 务必, 标明, 明文, 文章, 原始, 出处, 和, 作者, 信息, 及, 本, 声明, 。		
		]]>
		</screen>
	</section>
	<section>
		<title>精准模式与搜索引擎模式比较</title>
		<programlisting>
		<![CDATA[
# encoding=utf-8
import jieba
import paddle
text = "小明硕士毕业于中国科学院计算所，后在日本京都大学深造"
segs = jieba.cut(text)  # 精准模式
print(", ".join(segs))
print("=" * 50)
searchs = jieba.cut_for_search(text)  # 搜索引擎模式
print(", ".join(searchs))
		]]>
		</programlisting>
		<para>输出结果</para>
		<screen>
		<![CDATA[
neo@MacBook-Pro-Neo ~/workspace/python/jieba % python3.9 /Users/neo/workspace/python/jieba/search.py
Building prefix dict from the default dictionary ...
Loading model from cache /var/folders/2f/jfnljdpn1t1dj_f61z2s8bwm0000gn/T/jieba.cache
Loading model cost 0.807 seconds.
Prefix dict has been built successfully.
小明, 硕士, 毕业, 于, 中国科学院, 计算所, ，, 后, 在, 日本京都大学, 深造
==================================================
小明, 硕士, 毕业, 于, 中国, 科学, 学院, 科学院, 中国科学院, 计算, 计算所, ，, 后, 在, 日本, 京都, 大学, 日本京都大学, 深造	
		]]>
		</screen>
	</section>
	<section>
		<title>词性标注</title>
		<screen>
		<![CDATA[
标签	含义		标签	含义		标签	含义		标签	含义
n	普通名词	f	方位名词	s	处所名词	t	时间
nr	人名		ns	地名		nt	机构名	nw	作品名
nz	其他专名	v	普通动词	vd	动副词	vn	名动词
a	形容词	ad	副形词	an	名形词	d	副词
m	数量词	q	量词		r	代词		p	介词
c	连词		u	助词		xc	其他虚词	w	标点符号
PER	人名		LOC	地名		ORG	机构名	TIME	时间
		
		]]>
		</screen>
		<para></para>
		<programlisting>
		<![CDATA[
import jieba
import jieba.posseg as pseg
import paddle
words = pseg.cut("我爱北京天安门")  # jieba默认模式
for word, flag in words:
    print('%s %s' % (word, flag))

print("="*40)

paddle.enable_static()
jieba.enable_paddle()  # 启动paddle模式。
words = pseg.cut("我爱北京天安门", use_paddle=True)  # paddle模式
for word, flag in words:
    print('%s %s' % (word, flag))
		
		]]>
		</programlisting>
		<para></para>
		<screen>
		<![CDATA[
neo@MacBook-Pro-Neo ~/workspace/python % python3.9 /Users/neo/workspace/python/jieba/seg.py
Building prefix dict from the default dictionary ...
Loading model from cache /var/folders/2f/jfnljdpn1t1dj_f61z2s8bwm0000gn/T/jieba.cache
Loading model cost 0.753 seconds.
Prefix dict has been built successfully.
我 r
爱 v
北京 ns
天安门 ns
========================================
Paddle enabled successfully......
我 r
爱 v
北京 LOC
天安门 LOC		
		]]>
		</screen>
	</section>
	<section>
		<title>词典管理</title>
		<section>
			<title>添加/删除词语</title>

			<programlisting>
		<![CDATA[
import jieba

text = "他来到了网易杭研大厦"

words = jieba.cut(text, HMM=False)
print(", ".join(words))
print("-" * 50)
jieba.add_word('杭研大厦')  # 将 “杭研大厦” 添加到词典
jieba.add_word('来到了')  # 将 “来到了” 添加到词典
words = jieba.cut(text, HMM=False)
print(", ".join(words))
print("-" * 50)
jieba.del_word('深圳')  # 将 “深圳” 从词典中删除
words = jieba.cut("我爱深圳", HMM=False)
print(", ".join(words))
		]]>
			</programlisting>
		</section>
		<section>
			<title>用户词典</title>
			<para>自定义词典</para>
			<programlisting>
			<![CDATA[
import jieba

jieba.load_userdict('dict.txt') # 载入用户词典
seg_list = jieba.cut("他来到了网易杭研大厦", HMM=False)  
print(", ".join(seg_list))

			]]>
			</programlisting>
		</section>
		<section>
			<title>自定义词库</title>
			<para>分词系统默认使用自带的词库，load_userdict 是在默认词库的基础上做加法操作。</para>
			<para>set_dictionary 是设置默认基础词库。</para>
			<programlisting>
			<![CDATA[
# encoding=utf-8
import jieba
jieba.set_dictionary('dict.txt')
seg_list = jieba.cut("转载请与作者联系，同时请务必标明文章原始出处和作者信息及本声明。")
print(seg_list)
print(", ".join(seg_list))			
			]]>
			</programlisting>
			<para>dict.txt</para>
			<programlisting>
			<![CDATA[
请务必 12 n
作者信息 10 n			
			]]>
			</programlisting>
		</section>
	</section>
	<section>
		<title>抽取文本标签</title>
		<section>
			<title>提取标签</title>
			<screen>
		<![CDATA[
方法参数：
jieba.analyse.extract_tags(sentence, topK=5, withWeight=True, allowPOS=())
参数说明：
sentence 需要提取的字符串，必须是str类型，不能是list
topK 提取前多少个关键字
withWeight 是否返回每个关键词的权重
allowPOS是允许的提取的词性，默认为allowPOS=‘ns’, ‘n’, ‘vn’, ‘v’，提取地名、名词、动名词、动词	
		]]>
			</screen>
			<programlisting>
			<![CDATA[
file = open('article.txt', 'r', encoding='utf-8')
contents = file.read()
print(jieba.analyse.extract_tags(sentence=contents, topK=20, allowPOS=('ns', 'n')))			
			]]>
			</programlisting>
		</section>
		<section>
			<title>基于 TextRank 算法的关键词抽取</title>
			<programlisting>
			<![CDATA[
import jieba.analyse
import jieba
import os

os.chdir('jieba')
file = open('article.txt', 'r', encoding='utf-8')
contents = file.read()
print(jieba.analyse.textrank(sentence=contents, topK=20, allowPOS=('ns', 'n')))			
			]]>
			</programlisting>
		</section>
	</section>
	<section>
		<title>返回词语在原文的起止位置</title>
		<programlisting>
		<![CDATA[
import jieba
import logging
logger = logging.getLogger()
jieba.default_logger = logger
text = u'大和服装饰品有限公司'
result = jieba.tokenize(text)
for tk in result:
    print("word %s\t\t start: %d \t\t end:%d" % (tk[0], tk[1], tk[2]))
print("-" * 50)
result = jieba.tokenize(text, mode='search')  # 搜索模式
for tk in result:
    print("word %s\t\t start: %d \t\t end:%d" % (tk[0], tk[1], tk[2]))
		
		]]>
		</programlisting>
		<para></para>
		<screen>
		<![CDATA[
		
		]]>
		</screen>
	</section>
</section>
<?xml version="1.0" encoding="UTF-8"?>
<section id="wordcloud">
	<title>wordcloud</title>
	<para>https://github.com/amueller/word_cloud</para>
	<screen>
		<![CDATA[
pip install wordcloud		
		]]>
	</screen>
	<para>演示</para>
	<programlisting>
		<![CDATA[
import wordcloud

w = wordcloud.WordCloud()
w.generate(
    "Netkiller Neo Linux Nginx SSH Ubuntu CentOS MySQL PostgreSQL Java Python")
w.to_file("wordcloud.png")		
		]]>
	</programlisting>
	<para>更多演示代码 <ulink url="https://amueller.github.io/word_cloud/auto_examples/index.html#example-gallery" /></para>
	<section id="wordcloud_cli">
		<title>wordcloud_cli</title>
		<programlisting>
		<![CDATA[
neo@MacBook-Pro-Neo ~ % wordcloud_cli -h
usage: wordcloud_cli [-h] [--text file] [--regexp regexp] [--stopwords file] [--imagefile file]
                     [--fontfile path] [--mask file] [--colormask file] [--contour_width width]
                     [--contour_color color] [--relative_scaling rs] [--margin width]
                     [--width width] [--height height] [--color color] [--background color]
                     [--no_collocations] [--include_numbers] [--min_word_length min_word_length]
                     [--prefer_horizontal ratio] [--scale scale] [--colormap map] [--mode mode]
                     [--max_words N] [--min_font_size size] [--max_font_size size]
                     [--font_step step] [--random_state seed] [--no_normalize_plurals] [--repeat]
                     [--version]

A simple command line interface for wordcloud module.

optional arguments:
  -h, --help            show this help message and exit
  --text file           specify file of words to build the word cloud (default: stdin)
  --regexp regexp       override the regular expression defining what constitutes a word
  --stopwords file      specify file of stopwords (containing one word per line) to remove from
                        the given text after parsing
  --imagefile file      file the completed PNG image should be written to (default: stdout)
  --fontfile path       path to font file you wish to use (default: DroidSansMono)
  --mask file           mask to use for the image form
  --colormask file      color mask to use for image coloring
  --contour_width width
                        if greater than 0, draw mask contour (default: 0)
  --contour_color color
                        use given color as mask contour color - accepts any value from
                        PIL.ImageColor.getcolor
  --relative_scaling rs
                        scaling of words by frequency (0 - 1)
  --margin width        spacing to leave around words
  --width width         define output image width
  --height height       define output image height
  --color color         use given color as coloring for the image - accepts any value from
                        PIL.ImageColor.getcolor
  --background color    use given color as background color for the image - accepts any value from
                        PIL.ImageColor.getcolor
  --no_collocations     do not add collocations (bigrams) to word cloud (default: add unigrams and
                        bigrams)
  --include_numbers     include numbers in wordcloud?
  --min_word_length min_word_length
                        only include words with more than X letters
  --prefer_horizontal ratio
                        ratio of times to try horizontal fitting as opposed to vertical
  --scale scale         scaling between computation and drawing
  --colormap map        matplotlib colormap name
  --mode mode           use RGB or RGBA for transparent background
  --max_words N         maximum number of words
  --min_font_size size  smallest font size to use
  --max_font_size size  maximum font size for the largest word
  --font_step step      step size for the font
  --random_state seed   random seed
  --no_normalize_plurals
                        whether to remove trailing 's' from words
  --repeat              whether to repeat words and phrases
  --version             show program's version number and exit		
		]]>
		</programlisting>
	</section>
	<section>
		<title>WordCloud 对象配置参数</title>
		<screen>
			<![CDATA[
w = wordcloud.WordCloud(<参数>)

参数	描述
width		指定词云对象生成图片的宽度，默认400像素
height		指定词云对象生成图片的高度，默认200像素
min_font_size	指定词云中字体的最小字号，默认4号
max_font_size	指定词云中字体的最大字号，根据高度自动调节
font_step	指定词云中字体字号的步进间隔，默认为1
font_path	指定字体文件的路径，默认None
max_words	指定词云显示的最大单词数量，默认200
stop_words	指定词云的排除词列表，即不显示的单词列表
mask		指定词云形状，默认为长方形，需要引用imread()函数
background_color	指定词云图片的背景颜色，默认为黑色			
			]]>
		</screen>
		<programlisting>
			<![CDATA[
import wordcloud

w = wordcloud.WordCloud(background_color="white")
w.generate(
    "Netkiller Neo Linux Nginx SSH Ubuntu CentOS MySQL PostgreSQL Java Python")
w.to_file("wordcloud.png")
			
			]]>
		</programlisting>
	</section>
	<section>
		<title>与分词共用</title>
		<programlisting>
			<![CDATA[
# encoding=utf-8
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import jieba
import jieba.analyse

# background = plt.imread('background.jpg')  # 遮罩图
content = open('text.txt', 'r').read()  # 生成词云的文档

# seg_list = jieba.lcut(f)  # 默认是精确模式
tags = jieba.analyse.extract_tags(content, topK=50)
text = ", ".join(list(tags))
print(text)

wordcloud = WordCloud(
    background_color='white',  # 背景颜色，根据图片背景设置，默认为黑色
    # mask = background, #笼罩图
    font_path='/Library/Fonts/AdobeSongStd-Light.otf',  # 若有中文需要设置才会显示中文
    width=1024,  # 宽度
    height=768,  # 高度
    margin=5  # 边缘空白
).generate(text)

plt.imshow(wordcloud)
plt.axis('off')
plt.show()
plt.close()

# 保存图片
wordcloud.to_file('wordcloud.jpg')
			
			]]>
		</programlisting>
	</section>
	<section>
		<title>遮罩图</title>
		<programlisting>
		<![CDATA[
# encoding=utf-8
import matplotlib.pyplot as plt
import numpy as np
from PIL import Image
from wordcloud import WordCloud
import jieba
import jieba.analyse

content = open('text.txt', 'r').read()
tags = jieba.analyse.extract_tags(content, topK=50)
text = ", ".join(list(tags))
# print(text)

# mask = np.array(Image.open("stormtrooper_mask.png"))
mask = np.array(Image.open("background.png"))

wordcloud = WordCloud(
    background_color='white',
    font_path='/Library/Fonts/AdobeSongStd-Light.otf',
    mask=mask,  # 遮罩图
    width=1024,  # 宽度
    height=768,  # 高度
    margin=5  # 边缘空白
).generate(text)
# 保存图片
wordcloud.to_file('wordcloud.jpg')

plt.imshow(wordcloud)
plt.axis('off')
plt.show()
plt.close()
		
		]]>
		</programlisting>
	</section>
</section>