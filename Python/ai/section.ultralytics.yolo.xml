<?xml version="1.0" encoding="UTF-8"?>
<section id="yolo">
	<title>YOLO</title>
	<orderedlist>
		<title>目标检测任务</title>
		<listitem>YOLO11：用于经典的目标检测任务。</listitem>
		<listitem>YOLO11-seg：用于实例分割，识别和分割图像中的对象。</listitem>
		<listitem>YOLO11-pose：用于关键点姿态估计，即确定人体的关键点（如关节位置）。</listitem>
		<listitem>YOLO11-obb：用于定向检测，可以识别并确定具有方向性物体的边界框（例如倾斜的目标物体）。</listitem>
		<listitem>YOLO11-cls：用于分类，负责对图像中的对象进行类别识别。</listitem>
	</orderedlist>
	<orderedlist>
		<title>模型规格</title>
		<listitem>YOLOv8 Nano 非常小 YOLOv8n</listitem>
		<listitem>YOLOv8 Small 小 YOLOv8s</listitem>
		<listitem>YOLOv8 Medium 中 YOLOv8m</listitem>
		<listitem>YOLOv8 Large 大 YOLOv8l</listitem>
		<listitem>YOLOv8 X（Extra Large） 非常大 YOLOv8x</listitem>
	</orderedlist>

	<section>
		<title>labels 文件格式</title>
		<para>x,y是目标的中心坐标，width,height是目标的宽和高。</para>
		<screen>
		<![CDATA[
<object-class> <x> <y> <width> <height>		
		]]>
		</screen>
		<para>VOC与YOLO之间的格式转换</para>
		<programlisting>
		<![CDATA[
# size是原图的（w,h), box是坐标

def voc_to_yolo(size, box):
    dw = 1./size[0]
    dh = 1./size[1]
    x = (box[0] + box[1])/2.0
    y = (box[2] + box[3])/2.0
    w = box[1] - box[0]
    h = box[3] - box[2]
    x = x*dw
    w = w*dw
    y = y*dh
    h = h*dh
    return (x,y,w,h)
 
def yolo_to_voc(size, box):
    x = box[0] * size[0]
    w = box[2] * size[0]
    y = box[1] * size[1]
    h = box[3] * size[1]
    xmin = int(x - w/2)
    xmax = int(x + w/2)
    ymin = int(y - h/2)
    ymax = int(y + h/2)
    return (xmin, ymin, xmax, ymax)
		
		]]>
		</programlisting>
	</section>

	<section id="yolo.train">
		<title>模型训练</title>
		<programlisting>
		<![CDATA[
from ultralytics import YOLO
# 指定模型
mode = YOLO('yolo11n-seg.pt')
# 让模型开始训练
mode.train(data='dataset.yaml',workers=0,epochs=10,batch=10)
# 验证模型
mode.val()		
		]]>
		</programlisting>
		<section>
			<title>训练参数</title>
			<para>训练设置所有参数详解</para>
			<para>YOLO 模型的训练设置包括在训练过程中使用的各种超参数和配置。这些设置会影响模型的性能、速度和准确性。关键的训练设置包括批量大小、学习率、动量和权重衰减。此外，优化器的选择、损失函数和训练数据集的组成也会影响训练过程。仔细调整和实验这些设置对于优化性能至关重要。</para>
			<screen>
			<![CDATA[
参数				默认值	描述
model			None	指定训练模型文件。接受 .pt 预训练模型或 .yaml 配置文件的路径。对定义模型结构或初始化权重至关重要。
data			None	数据集配置文件的路径（如 coco8.yaml）。该文件包含数据集特定参数，包括训练和验证数据的路径、类别名称和类别数量。注意：该yaml文件中必须至少包含训练集和验证集数据路径。
epochs			100		总训练 epochs 数。每个 epoch 代表完整遍历整个数据集一次。调整此值会影响训练时长和模型性能。
time			None	最大训练时间（小时）。如果设置，将覆盖 epochs 参数，允许在指定时间后自动停止训练。适用于时间受限的训练场景。
patience		100		在验证指标没有改进的情况下等待的 epochs 数，之后提前停止训练。通过在性能停滞时停止训练来防止过拟合。
batch			16		批量大小，有三种模式：设置为整数（如 batch=16），自动模式为 GPU 内存利用率的 60%（batch=-1），或指定利用率分数的自动模式（batch=0.70）。当你不会根据图像大小计算所需内存时，后面两种模式很有用。
imgsz			640		训练的目标图像大小。所有图像在输入模型前都将调整为此尺寸。影响模型准确性和计算复杂性。
save			True	启用训练检查点和最终模型权重的保存。对于恢复训练或模型部署非常有用。
save_period		-1		模型检查点保存频率，以 epochs 为单位。值为 -1 禁用此功能。在长时间训练期间保存中间模型非常有用。
cache			False	启用数据集图像缓存到内存（True/ram）、磁盘（disk）或禁用（False）。通过减少磁盘 I/O 提高训练速度，但增加内存使用。
device			None	指定训练的计算设备：单个 GPU（device=0），多个 GPU（device=0,1），CPU（device=cpu），或 Apple 芯片的 MPS（device=mps）。
workers			8		数据加载的工作线程数（如果是多 GPU 训练，则按 RANK 计算）。影响数据预处理和输入模型的速度，尤其在多 GPU 设置中。
project			None	保存训练输出的项目目录名称。允许有组织地存储不同实验。
name			None	训练运行的名称。用于在项目文件夹内创建子目录，保存训练日志和输出。
exist_ok		False	如果为 True，允许覆盖现有的项目/名称目录。适用于无需手动清理先前输出的迭代实验。
pretrained		True	决定是否从预训练模型开始训练。可以是布尔值或字符串路径，指定从哪个模型加载权重。增强训练效率和模型性能。
optimizer		'auto'	训练的优化器选择。选项包括 SGD、Adam、AdamW、NAdam、RAdam、RMSProp 等，或根据模型配置自动选择。影响收敛速度和稳定性。
verbose			False	启用详细输出，在训练期间提供详细日志和进度更新。适用于调试和密切监控训练过程。
seed			0		设置训练的随机种子，确保在相同配置下结果的可重复性。
deterministic	True	强制使用确定性算法，确保可重复性，但可能因限制非确定性算法而影响性能和速度。
single_cls		False	在训练期间将多类数据集中的所有类视为单一类。适用于二分类任务或专注于对象存在而非分类时。
rect			False	启用矩形训练，优化批次组成以最小化填充。可以提高效率和速度，但可能影响模型准确性。
cos_lr			False	使用余弦学习率调度器，按余弦曲线调整学习率。帮助更好地管理学习率以实现更好的收敛。
close_mosaic	10		在最后 N 个 epochs 中禁用马赛克数据增强，以在完成前稳定训练。设置为 0 禁用此功能。
resume			False	从上次保存的检查点恢复训练。自动加载模型权重、优化器状态和 epoch 计数，继续无缝训练。
amp				True	启用自动混合精度（AMP）训练，减少内存使用并可能加快训练速度，对准确性的影响最小。
fraction		1.0		指定用于训练的数据集的比例。允许在数据集的子集中进行训练，适用于实验或资源有限时。
profile			False	启用训练期间 ONNX 和 TensorRT 速度的分析，优化模型部署。
freeze			None	冻结模型的前 N 层或按索引指定的层，减少可训练参数数量。适用于微调或迁移学习。
lr0				0.01	初始学习率（如 SGD=1E-2，Adam=1E-3）。调整此值对优化过程至关重要，影响模型权重的更新速度。
lrf				0.01	最终学习率作为初始率的分数 = (lr0 * lrf)，与调度器一起使用以随时间调整学习率。
momentum		0.937	SGD 的动量因子或 Adam 的 beta1，影响当前更新中过去梯度的合并。
weight_decay	0.0005	L2 正则化项，惩罚大权重以防止过拟合。
warmup_epochs	3.0		学习率预热的 epochs 数，从低值逐渐增加到初始学习率，以在早期稳定训练。
warmup_momentum	0.8		预热阶段的初始动量，在预热期内逐渐调整到设定动量。
warmup_bias_lr	0.1		预热阶段偏置参数的学习率，有助于在初始 epochs 中稳定模型训练。
box				7.5		损失函数中 box 损失组件的权重，影响准确预测边界框坐标的重要性。
cls				0.5		总损失函数中分类损失的权重，影响正确类别预测相对于其他组件的重要性。
dfl				1.5		分布焦点损失的权重，用于某些 YOLO 版本的细粒度分类。
pose			12.0	在姿态估计训练的模型中姿态损失的权重，影响准确预测姿态关键点的重要性。
kobj			2.0		姿态估计模型中关键点对象性的损失权重，平衡检测置信度与姿态准确性。
label_smoothing	0.0		应用标签平滑，将硬标签软化为目标标签和标签间的均匀分布的混合，可以改善泛化。
nbs				64		用于损失归一化的名义批大小。
overlap_mask	True	决定在训练期间分割掩码是否应重叠，适用于实例分割任务。
mask_ratio		4		分割掩码的下采样率，影响训练期间使用的掩码分辨率。
dropout			0.0		分类任务的正则化丢弃率，通过在训练期间随机忽略单元来防止过拟合。
val				True	启用训练期间的验证，允许定期评估模型在单独数据集上的性能。
plots			False	生成并保存训练和验证指标的图表以及预测示例，为模型性能和学习进展提供视觉洞察。
			]]>
			</screen>
			
		</section>
		<section>
			<title>图像增强设置</title>
			<screen>
			<![CDATA[
参数		类型		默认值	范围				描述
hsv_h	float	0.015	0.0 - 1.0		调整图像的色调，增加颜色变化，有助于模型在不同光照条件下泛化。
hsv_s	float	0.7		0.0 - 1.0		改变图像的饱和度，影响颜色的强度。用于模拟不同的环境条件。
hsv_v	float	0.4		0.0 - 1.0		修改图像的亮度，有助于模型在各种光照条件下表现良好。
degrees	float	0.0		-180 - +180		随机旋转图像，提升模型识别不同方向物体的能力。
translate	float	0.1	0.0 - 1.0		水平和垂直平移图像，帮助模型学会检测部分可见的物体。
scale	float	0.5		>=0.0			按比例缩放图像，模拟不同距离的物体。
shear	float	0.0		-180 - +180		按指定角度剪切图像，模拟从不同角度观看物体的效果。
perspective	float	0.0	0.0 - 0.001		对图像应用随机透视变换，增强模型理解三维空间中物体的能力。
flipud	float	0.0		0.0 - 1.0		以指定概率上下翻转图像，增加数据多样性而不影响物体特性。
fliplr	float	0.5		0.0 - 1.0		以指定概率左右翻转图像，有助于学习对称物体，并增加数据集多样性。
bgr		float	0.0		0.0 - 1.0		以指定概率将图像通道从 RGB 翻转为 BGR，提高对错误通道顺序的鲁棒性。
mosaic	float	1.0		0.0 - 1.0		将四张训练图像组合成一张，模拟不同场景构图和物体交互。非常有效的复杂场景理解。
mixup	float	0.0		0.0 - 1.0		将两张图像及其标签混合，创建复合图像。通过引入标签噪声和视觉变化，提高模型的泛化能力。
copy_paste	float	0.0	0.0 - 1.0		将一个图像中的物体复制并粘贴到另一个图像中，有助于增加物体实例并学习物体遮挡。
auto_augment	str		randaugment	-	自动应用预定义的增强策略（randaugment、autoaugment、augmix），通过多样化视觉特征优化分类任务。
erasing	float	0.4		0.0 - 0.9		在分类训练期间随机擦除图像的一部分，鼓励模型关注较不明显的特征。
crop_fraction	float	1.0	0.1 - 1.0	将分类图像裁剪为其大小的一部分，以强调中心特征并适应物体尺度，减少背景干扰。
			]]>
			</screen>
			<para>用户可以根据自身的需求，设置数据集和任务的具体参数。通过不同值的调整，找到最佳的策略，从而达到最佳的模型训练效果。</para>
		</section>
		<section>
			<title></title>
			<screen>
			<![CDATA[
			
			]]>
			</screen>
			<programlisting>
			<![CDATA[
			
			]]>
			</programlisting>
		</section>
		<section>
			<title>输出结果</title>
			<screen>
			<![CDATA[
      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
     64/100     0.679G     0.5564     0.5321      1.084          9        320: 100%|██████████| 231/231 [00:25<00:00,  8.97it/s]
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 34/34 [00:05<00:00,  6.49it/s]
                   all       1068       1068      0.958      0.943      0.984      0.944			
			]]>
			</screen>
			<para>Epoch GPU_mem box_loss cls_loss dfl_loss Instances Size</para>
			<glosslist>
				<glossentry>
					<glossterm>Epoch</glossterm>
					<glossdef>
						<para>训练过程中的迭代次数，在机器学习和深度学习中，一个epoch是训练过程中的一个完整周期，包括了训练数据集中的所有样本，以逐渐优化其性能。
						</para>
					</glossdef>
				</glossentry>
				<glossentry>
					<glossterm>GPU_mem</glossterm>
					<glossdef>
						<para>GPU显存的使用情况，也就是图形处理器在训练过程中所使用的显存量。</para>
					</glossdef>
				</glossentry>
				<glossentry>
					<glossterm>box_loss</glossterm>
					<glossdef>边界框（bounding
						box）的平均损失，也就是模型预测的边界框与真实边界框之间的差异。在目标检测任务中，模型需要预测目标的准确位置，这通常通过边界框来实现。box_loss用于衡量模型在预测边界框位置时的准确性，并在训练过程中通过反向传播进行优化。
					</glossdef>
				</glossentry>
				<glossentry>
					<glossterm>cls_loss</glossterm>
					<glossdef>分类损失，也就是模型在预测目标类别时的错误程度。</glossdef>
				</glossentry>
				<glossentry>
					<glossterm>dfl_loss</glossterm>
					<glossdef>dfl_loss 代表某种特定于模型或实现方式的损失函数，这种损失函数与模型中的某个特定组件或功能相关。
					</glossdef>
				</glossentry>
				<glossentry>
					<glossterm>Instances</glossterm>
					<glossdef>在训练批次中检测到的目标实例数量。</glossdef>
				</glossentry>
				<glossentry>
					<glossterm>Size</glossterm>
					<glossdef>图像尺寸</glossdef>
				</glossentry>
			</glosslist>
			<para>Class Images Instances Box(P R mAP50 mAP50-95):</para>
			<glosslist>
				<title></title>
				<glossentry>
					<glossterm>Class</glossterm>
					<glossdef>检测目标类别</glossdef>
				</glossentry>
				<glossentry>
					<glossterm>Images</glossterm>
					<glossdef>测试集中包含的图像数量</glossdef>
				</glossentry>
				<glossentry>
					<glossterm>Instances</glossterm>
					<glossdef></glossdef>
				</glossentry>
				<glossentry>
					<glossterm>P</glossterm>
					<glossdef>预测精准度，正确预测数量占比</glossdef>
				</glossentry>
				<glossentry>
					<glossterm>R</glossterm>
					<glossdef>召回率</glossdef>
				</glossentry>
				<glossentry>
					<glossterm>mAP50</glossterm>
					<glossdef>平均精度，阈值0.5</glossdef>
				</glossentry>
				<glossentry>
					<glossterm>mAP50-95</glossterm>
					<glossdef>法制从0.5~0.95范围之间的平均精度</glossdef>
				</glossentry>
			</glosslist>

		</section>
		<section>
			<title>训练结果</title>
			<section>
				<title>weights 文件夹</title>
				<para>打开 weights 文件夹里面有两个文件，分别是：best.pt和last.pt。</para>
				<para>best.pt 是整个训练过程中，性能最好的模型权重文件。</para>
				<para>last.pt 是最后一次训练的模型权重文件。</para>
				<para>一般我们选择 best.pt 文件，用于生产环境。</para>
			</section>
			<section>
				<title>args.yaml 文件</title>
				<screen>
				<![CDATA[
task: detect
mode: train
model: tongue.yaml
data: datasets/test/YOLODataset/dataset.yaml
epochs: 100
time: null
patience: 100
batch: 16
imgsz: 320
save: true
save_period: -1
cache: false
device: 0
workers: 8
project: null
name: train9
exist_ok: false
pretrained: true
optimizer: auto
verbose: true
seed: 0
deterministic: true
single_cls: false
rect: false
cos_lr: false
close_mosaic: 10
resume: false
amp: true
fraction: 1.0
profile: false
freeze: null
multi_scale: false
overlap_mask: true
mask_ratio: 4
dropout: 0.0
val: true
split: val
save_json: false
save_hybrid: false
conf: null
iou: 0.7
max_det: 300
half: false
dnn: false
plots: true
source: null
vid_stride: 1
stream_buffer: false
visualize: false
augment: false
agnostic_nms: false
classes: null
retina_masks: false
embed: null
show: false
save_frames: false
save_txt: false
save_conf: false
save_crop: false
show_labels: true
show_conf: true
show_boxes: true
line_width: null
format: torchscript
keras: false
optimize: false
int8: false
dynamic: false
simplify: true
opset: null
workspace: null
nms: false
lr0: 0.01
lrf: 0.01
momentum: 0.937
weight_decay: 0.0005
warmup_epochs: 3.0
warmup_momentum: 0.8
warmup_bias_lr: 0.1
box: 7.5
cls: 0.5
dfl: 1.5
pose: 12.0
kobj: 1.0
nbs: 64
hsv_h: 0.015
hsv_s: 0.7
hsv_v: 0.4
degrees: 0.0
translate: 0.1
scale: 0.5
shear: 0.0
perspective: 0.0
flipud: 0.0
fliplr: 0.5
bgr: 0.0
mosaic: 1.0
mixup: 0.0
copy_paste: 0.0
copy_paste_mode: flip
auto_augment: randaugment
erasing: 0.4
crop_fraction: 1.0
cfg: null
tracker: botsort.yaml
save_dir: D:\workspace\netkiller\runs\detect\train
				]]>
				</screen>
			</section>
			<section>
				<title>confusion_matrix.png / confusion_matrix_normalized.png
				</title>
				<para>混淆矩阵的归一化版本</para>
			</section>

			<section>
				<title>P_curve.png / R_curve.png/ PR_curve.png /F1_curve.png</title>
				<orderedlist>
					<listitem>P_curve.png 每个 label 的精度</listitem>
					<listitem>R_curve.png 每个 label 的召回率</listitem>
					<listitem>PR_curve.png 精度与召回互相妥协的曲线。范围 0.0 ~ 1.0 越接近 1.0 效果越好。
					</listitem>
				</orderedlist>
			</section>

			<section>
				<title>labels.jpg / labels_correlogram.jpg</title>
				<para>labels.jpg 分类标签的分布</para>
				<para></para>
			</section>

			<section>
				<title>results.png / results.csv</title>
				<para>先看前三列是 loss
					损失率图表，X轴表示训练轮次，Y轴表示损失值。推理结果和人工标注的答案之间的差异，称为“损失”。损失越小越好，损失为0则说明推理和正确答案之间没有差异，即预测100%命中。
				</para>
				<orderedlist>
					<title>train 训练集，val 验证集</title>
					<listitem>train/box_loss和val/box_loss</listitem>
					<listitem>train/cls_loss和val/cls_loss</listitem>
					<listitem>train/dfl_loss和val/dfl_loss</listitem>
				</orderedlist>
				<orderedlist>
					<title>精度/召回率</title>
					<listitem>metric/precision(B) 精度</listitem>
					<listitem>metrics/recall(B) 召回率</listitem>
				</orderedlist>
				<para>mAP50是重合度以50%为界限的平均精度。而mAP50-95则是IoU阈值从50%到95%范围内的平均值</para>
				<para>X轴是训练轮次，Y表示精度，取值0.0 ~ 1.0（100%）越接近1.0效果越好。</para>
			</section>
			<section>
				<title>train_batch0.jpg</title>
			</section>
			<section>
				<title>val_batch0_labels.jpg / val_batch0_pred.jpg</title>
				<para>评估对照组，对比两个图片，可以看到训练结果，标签是否都正确预测。</para>
			</section>
		</section>
		<section>
			<title>恢复中断的训练</title>
			<para>如果训练过程异常中断，可以通过设置 resume=True 选项，读取训练过程，断点信息存储在 ‘path/to/last.pt’ 文件中，启动后会从之前的断点恢复训练，</para>
			<programlisting>
			<![CDATA[
from ultralytics import YOLO

# 加载模型
model = YOLO("path/to/last.pt")  # 加载训练的模型

# 恢复训练
results = model.train(resume=True)
			
			]]>
			</programlisting>

		</section>

	</section>
	<section id="yolo.val">
		<title>模型评估</title>
		<programlisting>
		<![CDATA[
from ultralytics import YOLO
# ---------- 加载模型 ----------
model = YOLO('runs/detect/train/weights/best.pt')
# ---------- 模型评估 ----------
model.val(data='captcha/images/YOLODataset/dataset.yaml')
		
		]]>
		</programlisting>
		<section>
			<title>选择数据集</title>
			<para>split='test' 可以选择数据集，默认是 val</para>
			<programlisting>
			<![CDATA[
from ultralytics import YOLO
 
# Load a model
model = YOLO('yolov11n.pt')  # load an official model
model = YOLO('runs/detect/train/weights/best.pt')  # load a custom model
 
# Validate the model
metrics = model.val(split='test')  # no arguments needed, dataset and settings remembered
metrics.box.map    # map50-95
metrics.box.map50  # map50
metrics.box.map75  # map75
metrics.box.maps   # a list contains map50-95 of each category			
			]]>
			</programlisting>
		</section>
		<section>
			<title>验证参数设置</title>
			<screen>
			<![CDATA[
参数	类型	默认值	描述
data		str		None	指定数据集配置文件的路径（例如，coco8.yaml）。这个文件包括验证数据的路径、类名和类的数量。
imgsz		int		640		定义输入图像的大小。所有图像在处理前都调整到这个尺寸。
batch		int		16		设置每个批次的图像数量。使用 -1 表示自动批量（AutoBatch），它会根据 GPU 内存可用性自动调整。
save_json	bool	False	如果为 True，将结果保存到 JSON 文件中，以便进一步分析或与其他工具集成。
save_hybrid	bool	False	如果为 True，保存标签的混合版本，将原始注释与额外的模型预测相结合。
conf		float	0.001	设置检测的最小置信阈值。丢弃置信度低于这个阈值的检测。
iou			float	0.6		设置非极大值抑制（NMS）的交并比（IoU）阈值。有助于减少重复检测。
max_det		int		300		限制每张图像的最大检测数量。在密集场景中用于防止过度检测。
half		bool	True	启用半精度（FP16）计算，减少内存使用，并可能在最小影响准确性的情况下提高速度。
device		str		None	指定验证的设备（cpu, cuda:0 等）。允许灵活使用 CPU 或 GPU 资源。
dnn			bool	False	如果为 True，使用 OpenCV DNN 模块进行 ONNX 模型推理，作为 PyTorch 推理方法的替代。
plots		bool	False	当设置为 True 时，生成并保存预测与真实值的对比图，以便直观评估模型的性能。
rect		bool	False	如果为 True，使用矩形推理进行批处理，减少填充，可能提高速度和效率。
split		str		val		确定用于验证的数据集划分（val, test, 或 train）。允许灵活选择用于性能评估的数据段。			
			]]>
			</screen>
		</section>
	</section>
	<section id="model.predict">
		<title>模型预测</title>
		<programlisting>
		<![CDATA[
from ultralytics import YOLO

# 加载预训练的YOLOv11n模型
model = YOLO("yolo11n.pt")  # yolo11n.pt, yolo11s.pt, yolo11m.pt, yolo11x.pt

# 对'bus.jpg'图像进行推理，并获取结果 https://ultralytics.com/images/bus.jpg
results = model.predict("https://ultralytics.com/images/bus.jpg", save=True, imgsz=320, conf=0.5)

# 处理返回的结果
for result in results:
    boxes = result.boxes  # 获取边界框信息
    probs = result.probs  # 获取分类概率
    result.show()  # 显示结果型的架构
    print(boxes)
    print(probs)		
		]]>
		</programlisting>
		<para>换一个模型，试试 yolo11n-seg.pt</para>
		<programlisting>
		<![CDATA[
from ultralytics import YOLO

model = YOLO("yolo11n-seg.pt")
results = model.predict("https://ultralytics.com/images/bus.jpg", save=True, imgsz=320, conf=0.5)

# 处理返回的结果
for result in results:
    boxes = result.boxes  # 获取边界框信息
    probs = result.probs  # 获取分类概率
    print(boxes)
    print(probs)
    result.show()  # 显示结果型的架构		
		]]>
		</programlisting>

		<section>
			<title>predict 参数</title>
			<para>推理参数</para>
			<screen>
			<![CDATA[
参数				类型			默认值	描述
source			str			‘ultralytics/assets’	指定推理的数据源。可以是图像路径、视频文件、目录、URL 或实时流的设备 ID。支持多种格式和来源，使不同类型的输入都能灵活应用。
conf			float		0.25	设置检测的最小置信度阈值。低于此阈值的检测对象将被忽略。调整此值有助于减少误报。
iou				float		0.7		非极大值抑制（NMS）的交并比（IoU）阈值。较低的值会通过消除重叠的框来减少检测数量，对于减少重复检测很有用。
imgsz			int/tuple	640		定义推理的图像大小。可以是一个整数 640，用于方形缩放，也可以是一个（高度，宽度）元组。适当的尺寸可以提高检测准确性和处理速度。
half			bool		False	启用半精度（FP16）推理，可以在支持 GPU 上加速模型推理，同时对准确度的影响最小。
device			str			None	指定推理的设备（例如，cpu, cuda:0 或 0）。允许用户在 CPU、特定 GPU 或其他计算设备之间选择模型执行。
max_det			int			300		每张图像允许的最大检测数量。限制模型在单个推理中可以检测到的总对象数量，以防止在密集场景中产生过多的输出。
vid_stride		int			1		视频输入的帧间跳过。允许在视频中跳过帧以提高处理速度，但代价是时间分辨率。值为 1 时处理每个帧，更高的值跳过帧。
stream_buffer	bool		False	确定在处理视频流时是否应缓冲所有帧（True），或者模型是否应返回最新的帧（False）。对于实时应用很有用。
visualize		bool		False	激活推理期间模型特征的可视化，提供模型“看到”的洞察。对于调试和模型解释非常有用。
augment			bool		False	启用预测的测试时间增强（TTA），可能会提高检测的鲁棒性，但代价是推理速度。
agnostic_nms	bool		False	启用类无关的非极大值抑制（NMS），合并不同类的重叠框。在多类检测场景中，当类之间经常有重叠时非常有用。
classes			list [int]	None	过滤预测到一组类 ID。只有属于指定类的检测结果才会被返回。对于在多类检测任务中关注相关对象非常有用。
retina_masks	bool		False	如果模型中可用，则使用高分辨率的分割掩码。这可以增强分割任务的掩码质量，提供更细的细节。
embed			list [int]	None	指定要从哪些层提取特征向量或嵌入。对于下游任务，如聚类或相似度搜索非常有用。			
			]]>
			</screen>
			<para>可视化参数</para>
			<screen>
			<![CDATA[
参数			类型		默认值	描述
show		bool	False	如果为 True，则在窗口中显示标注的图像或视频。在开发或测试期间，这对于立即获得视觉反馈非常有用。
save		bool	False	启用将标注的图像或视频保存到文件。这对于文档、进一步分析或分享结果非常有用。
save_frames	bool	False	在处理视频时，将单独的帧保存为图像。这对于提取特定帧或进行详细的逐帧分析非常有用。
save_txt	bool	False	将检测结果保存为文本文件，格式为 [class] [x_center] [y_center] [width] [height] [confidence]。这对于与其他分析工具集成非常有用。
save_conf	bool	False	在保存的文本文件中包含置信度分数。增强了可用于后处理和分析的详细信息。
save_crop	bool	False	保存检测的裁剪图像。这对于数据增强、分析或创建特定对象的聚焦数据集非常有用。
show_labels	bool	True	在视觉输出中显示每个检测的标签。提供了对检测到的对象立即的理解。
show_conf	bool	True	在标签旁边显示每个检测的置信度分数。为每个检测提供了模型确定性的洞察。
show_boxes	bool	True	在图像或视频帧中检测到的对象周围绘制边界框。这对于视觉识别和定位图像或视频帧中的对象至关重要。
line_width	int		None	指定边界框的线条宽度。如果为 None，线条宽度将根据图像大小自动调整。为清晰度提供了视觉定制。
			
			]]>
			</screen>
			<programlisting>
			<![CDATA[
from yolov8 import YOLO
 
# 加载预训练模型
model = YOLO('yolov8.pt')
 
# 执行预测
results = model.predict(
    source='path/to/image.jpg',  # 输入源，可以是图像路径、视频路径、摄像头索引等
    conf=0.5,                    # 置信度阈值，滤除低于该置信度的检测结果。
    iou=0.45,                    # NMS的交并比阈值，用于消除重叠的检测框。
    device='cuda',               # 使用GPU
    imgsz=(640, 640),            # 输入图像的尺寸，可以是单个整数（长宽相等）或元组（长和宽）。
    half=False,                  # 不使用半精度浮点数
    save_txt=True,               # 保存预测结果为文本文件
    save_conf=True,              # 在保存的文本文件中包含置信度
    save_crop=False,             # 不保存裁剪后的检测结果
    save_img=True,               # 保存带有检测框的图像
    classes=[0, 1, 2],           # 仅检测指定的类
    agnostic_nms=False,          # 不使用类别无关的NMS
    augment=False,               # 不使用数据增强
    visualize=False,             # 不可视化特征图
    project='runs/detect',       # 保存预测结果的项目目录
    name='exp',                  # 保存预测结果的子目录名称
    exist_ok=False,              # 不允许项目目录存在
    line_thickness=3,            # 绘制检测框的线条粗细
    hide_labels=False,           # 显示检测框的标签
    hide_conf=False              # 显示检测框的置信度
)
 
# 输出预测结果
print(results)
			
			]]>
			</programlisting>
		</section>
		<section>
			<title>返回结果</title>
			<para>返回结果通常包含以下主要属性和方法：</para>
			<literallayout>
			<![CDATA[
1.boxes: 包含检测到的边界框信息的对象。
2.scores: 包含每个检测目标的置信度分数。
3.classes: 包含每个检测目标的类别索引。
4.masks: 如果使用实例分割模型，包含分割掩码。
5.keypoints: 如果使用关键点检测模型，包含关键点信息。
6.orig_img: 原始输入图像。
7.plot(): 用于可视化检测结果的方法。			
			]]>
			</literallayout>
			<para>boxes: </para>
			<literallayout>
			<![CDATA[
1.boxes.xyxy: 返回每个检测目标的边界框坐标，格式为 [x_min, y_min, x_max, y_max]。
2.boxes.conf: 返回每个检测目标的置信度分数。
3.boxes.cls: 返回每个检测目标的类别索引。			
			]]>
			</literallayout>
		</section>
		<section>
			<title>提取标签</title>
			<programlisting>
			<![CDATA[
from ultralytics import YOLO

# 加载模型
# model = YOLO("yolo11n.pt")
# model = YOLO('weights/last.pt')
model = YOLO('weights/best.pt')

# 模型预测，save=True 的时候表示直接保存yolov8的预测结果
results = model('images/0_5902.png')

# View results
for result in results:
    # 处理结果
    boxes = result.boxes  # 边界框结果
    masks = result.masks  # 分割掩码结果
    names = result.names
    keypoints = result.keypoints  # 关键点检测结果
    probs = result.probs  # 分类概率结果
    obb = result.obb  # 方向边界框结果（OBB）
    print(boxes)
    for box in boxes:
        print(names[int(box.cls)])
    result.show()  # 显示结果			
			]]>
			</programlisting>
		</section>
		<section>
			<title>保存识别图像</title>
			<programlisting>
			<![CDATA[
@app.post("/netkiller")
async def netkiller(file: bytes = File()):
    try:
        filename = f"tmp/{uuid.uuid4()}.png"
        print(filename)
        with open(filename, "wb") as f:
            f.write(file)

        results = model(filename) # , conf=0.45

        for result in results:
            boxes = result.boxes  # 获取边界框信息
            # probs = result.probs  # 获取分类概率
            names = result.names
            data = result.boxes.data
            print(boxes)
            # print(probs)
            if names is not None:
                labels = []
                for box in boxes:
                    label = names[int(box.cls)]
                    labels.append(label)
                    print(label)
                # result.show()  # 显示结果型的架构
                result.save(filename="tmp/test.png")

            if boxes is not None:
                image = Image.open(filename)
                # x0, y0, x1, y1 = map(int, boxes.xyxy[0])
                crop = tuple(map(int, boxes.xyxy[0]))

                region = image.crop(crop)
                region.save('tmp/neo.jpg')

            return {"label": labels}
    except Exception as e:
        log.error(e)
    return {"label": None}			
			]]>
			</programlisting>
		</section>
		<section id="result.boxes">
			<title>boxes</title>
			<para>tensor([ 59.0264, 88.7340, 367.5180, 420.6993]) 转 list [ 59.0264, 88.7340, 367.5180, 420.6993]</para>
			<programlisting>
			<![CDATA[
xyxy = boxes.xyxy[0].tolist()			
			]]>
			</programlisting>
			<para>boxes.data 数据，YOLO 边界框格式：[x1, y1, x2, y2, confidence, class]</para>
			<programlisting>
			<![CDATA[
boxes = result.boxes.data.cpu().numpy()
print(result.boxes.data.tolist())		
			]]>
			</programlisting>
			<programlisting>
			<![CDATA[
    def boxes(self, source:str,target:str):
        if not os.path.exists(source):
            return None

        try:

            image = cv2.imread(source)
            if image is None:
                return None

            results = self.model(source,verbose=False)

            for result in results:
                # 提取检测到的目标的边界框
                boxes = result.boxes.data.cpu().numpy()  # YOLO 边界框格式：[x1, y1, x2, y2, confidence, class]
                # print(result.boxes.data.tolist())
                if self.args.output:
                    result.save(filename=os.path.join(self.args.output, os.path.basename(source)))
                    result.save_crop(save_dir=os.path.join(self.args.output,'crop'), file_name="detection")
                # print(boxes)

                for idx, box in enumerate(boxes):
                    x1, y1, x2, y2, conf, cls = map(int, box[:6])
                    cropped = image[y1:y2, x1:x2]
                    output = os.path.join(self.args.target, f"{os.path.splitext(os.path.basename(source))[0]}_{idx}.jpg")
                    cv2.imwrite(output, cropped)
                    # print(f"Saved cropped image: {output}")
        except Exception as e:
            # log.error(e)
            print("boxes: ",e)
            exit()
        return None		
			]]>
			</programlisting>
						
		</section>
		<section>
			<title>verbose 打印详细输出</title>
			<para>默认会输出</para>
			<screen>
			<![CDATA[
image 1/1 /Users/neo/PycharmProjects/netkiller/tmp/test/20150917_082555.jpg: 320x192 1 shebei, 11.2ms
Speed: 0.5ms preprocess, 11.2ms inference, 0.5ms postprocess per image at shape (1, 3, 320, 192)
			]]>
			</screen>
			<para>关闭输出</para>
			<programlisting>
			<![CDATA[
model(img, save=True, verbose=False)			
			]]>
			</programlisting>
		</section>
	</section>
	<section id="model.export">
		<title>模型导出</title>
		<section>
			<title>onnx 格式</title>
			<screen>
			<![CDATA[
(.venv) neo@Neo-Mac-mini netkiller % pip install onnx onnxslim -i https://pypi.tuna.tsinghua.edu.cn/simple		
			]]>
			</screen>
			<programlisting>
			<![CDATA[
from ultralytics import YOLO

model = YOLO("yolo11n.pt")
model.export(format="onnx")

# 预测看下效果
onnx_model = YOLO("yolo11n.onnx", task="detect")
results = onnx_model("https://ultralytics.com/images/bus.jpg")
results[0].show()		
			]]>
			</programlisting>
		</section>
		<section>
			<title>engine 格式</title>
			<programlisting>
			<![CDATA[
from ultralytics import YOLO

model = YOLO("yolo11n.pt")
model.export(format="engine")
tensorrt_model = YOLO("yolo11n.engine", task="detect")
results = tensorrt_model("https://ultralytics.com/images/bus.jpg")
results[0].show()			
			]]>
			</programlisting>
		</section>

	</section>
</section>