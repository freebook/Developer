<?xml version="1.0" encoding="UTF-8"?>
<section>
	<title>什么是归一化？</title>
	<para>归一化是数据预处理中的一种常用技术，旨在将数据按比例缩放，使之落入一个小的特定区间，通常是[0, 1]或[-1, 1]。</para>
	<section>
		<title>最小-最大 归一化（Min-Max Scaling）</title>
		<para>计算方法</para>
		<screen>
		<![CDATA[
num = [2.5,3.5,0.5,1.5]

for n in num:
    result = (n-min(num))/(max(num)-min(num))
    print(round(result,4), end=' ')		
		]]>
		</screen>
	</section>
</section>
<section>
	<title>什么是残差</title>
	<para>在数学和统计学领域，残差（Residual）是指观测值与预测值（或拟合值）之间的差异。设观测值为，预测值为，那么残差。例如，我们通过一个线性回归模型来预测房屋价格。对于某一套房子，实际价格（观测值）是 200 万元，而模型预测出来的价格（预测值）是 180 万元，那么残差就是万元。</para>
	<screen>
	<![CDATA[
残差在回归分析中的应用
评估模型拟合优度：残差可以帮助我们了解模型对数据的拟合程度。如果残差较小，说明模型能够较好地拟合数据。例如，在简单线性回归中，我们可以绘制残差图（以预测值为横坐标，残差为纵坐标）。如果残差在零点附近随机分布，没有明显的规律（如系统性的上升或下降趋势、聚集在某个区域等），这表明模型拟合得比较好。相反，如果残差呈现出某种规律性，比如残差随着预测值的增加而增大，那就意味着模型可能存在问题，比如遗漏了重要的变量或者模型的形式不适合数据。
检测异常值：残差较大的观测点可能是异常值。继续以房屋价格预测为例，如果大部分房子的残差都在 - 10 万元到 10 万元之间，但是有一套房子的残差达到了 100 万元，这就需要我们进一步检查这个观测点。它可能是因为数据录入错误，或者这套房子有一些特殊的属性（如独特的地理位置、豪华的装修等）没有被模型考虑到。

残差在时间序列分析中的应用
在时间序列分析中，残差用于衡量模型对时间序列数据的预测准确性。例如，我们用一个 ARIMA 模型（自回归移动平均模型）来预测某产品的销售量。残差可以帮助我们判断模型是否能够很好地捕捉时间序列的波动特征。如果残差序列中存在明显的自相关性（即残差的当前值与过去值之间存在关联），这可能意味着模型没有充分利用数据中的信息，需要进一步调整模型的参数或者结构。

残差在机器学习中的应用
在机器学习模型评估中，残差也是一个重要的概念。以神经网络模型为例，训练集和测试集上的残差大小可以反映模型的泛化能力。如果模型在训练集上残差很小，但在测试集上残差很大，这可能是出现了过拟合现象。也就是说，模型过度学习了训练集中的细节，而不能很好地推广到新的数据上。我们可以通过一些技术，如正则化等，来减少过拟合，使得模型在测试集上的残差也能保持在一个合理的范围内。	
	]]>
	</screen>
</section>

<section>
	<title>网络模型的选择</title>
	<para>随着深度学习的火热，计算机视觉领域内的卷积神经网络模型也层出不穷。</para>
	<para>所以模型不断推陈出新，有些模型已经很老，甚至已经淘汰，不会再被用在生产环境，我们也不必去学习。</para>
	<graphic format="png" fileref="images/ai/timeline.webp"
		width="100%" srccredit="neo" />
	<para>例如做图像识别，建议直接使用 yolo11 不要再研究 VGG,LeNet,AlexNet
		这种古董。新模型建立在新技术基础之上，伴随着性能提升等等</para>
</section>
<section>
	<title>二分类</title>
	<para>人工智能深度学习中的二分类问题是一种基本的分类任务。它是指将输入的数据样本划分为两个互斥的类别。例如，在判断一封邮件是垃圾邮件还是正常邮件时，只有这两种类别选项，这就是一个典型的二分类问题。再比如，医学影像中判断一个肿瘤是良性还是恶性，也是二分类问题。</para>
</section>
<section>
	<title>多分类</title>
	<para>多分类（Multi - classification）是机器学习和数据挖掘领域中的一种分类任务类型。多分类问题是在二分类问题的基础上扩展而来的，与二分类（Binary - classification）将数据分为两个类别不同，多分类是指将输入的数据样本划分到多于两个互斥的类别中。例如，在手写数字识别任务中，要把输入的手写数字图像分类到 0 - 9 这 10 个类别中；在自然语言处理的文本情感分类中，可能将文本情感分为 “非常喜欢”“喜欢”“中立”“不喜欢”“非常不喜欢” 等多个类别。</para>
	<para>在深度学习中，常用的多分类算法包括K近邻(KNN)、决策树、朴素贝叶斯等，我们通常使用一种称为“softmax”的激活函数来实现多分类。softmax函数可以将神经网络的输出映射到[0,1]的范围内，并保证所有输出之和为1，从而得到每个类别的概率分布。</para>
</section>
<section>
	<title>多标签</title>
	<para>人工智能多标签技术是一种机器学习技术，它主要用于处理一个样本可能同时属于多个不同类别标签的情况。与传统的单标签分类不同，单标签分类任务中每个样本只能被分配到一个类别，而多标签分类允许一个样本有多个相关的类别标签。例如，在图像分类中，一张照片可能同时包含 “山脉”“河流”“森林” 等多个元素，那么这张照片就可以用多标签技术标记为这三个类别。</para>
</section>
<section>
	<title>多任务分类</title>
	<para>在人工智能深度学习领域，多任务分类是指一个模型同时处理多个不同但相关的分类任务。例如，在一个图像识别系统中，模型可能同时要对图像中的物体进行类别分类（如判断是汽车、自行车还是行人），还要对物体的颜色进行分类（如红色、蓝色、绿色等）以及对物体的大小（如大、中、小）进行分类。与单任务分类只专注于一个分类目标不同，多任务分类试图利用任务之间的相关性来提高模型的效率和泛化能力。</para>
</section>
<section>
	<title>向量数据处理</title>
	<section id="tokenizers">
		<title>tokenizers</title>
		<section>
			<title>Normalization</title>
			<para>文本清洗，Normalization 对原始文本 sentence 执行一系列清洗操作，如：删除空格、去除重音字符、小写化
			</para>
			<programlisting>
		<![CDATA[
from tokenizers import normalizers
from tokenizers.normalizers import NFD, StripAccents

normalizer = normalizers.Sequence([NFD(), StripAccents()])
text = normalizer.normalize_str("Héllò hôw are ü?")
print(text)
# "Hello how are u?"		
		]]>
			</programlisting>
		</section>
		<section>
			<title>Pre-Tokenization</title>
			<para>拆分文本，并标记文本的位置</para>
			<programlisting>
		<![CDATA[
from tokenizers.pre_tokenizers import Whitespace
from tokenizers import pre_tokenizers
from tokenizers.pre_tokenizers import Digits

#Whitespace使用正则表达式\w+|[^\w\s]+，即以word开头，以空格或非word结尾来拆分token，返回数据  List[Tuple[str, Offsets]]:
pre_tokenizer = Whitespace()
data1 = pre_tokenizer.pre_tokenize_str("What's your nickname? My nickname is netkiller.")
print(data1)

pre_tokenizer = pre_tokenizers.Sequence([Whitespace(), Digits(individual_digits=True)])
data2 = pre_tokenizer.pre_tokenize_str("https://www.netkiller.cn")
print(data2)		
		]]>
			</programlisting>
			<para>输出结果</para>
			<screen>
		<![CDATA[
[('What', (0, 4)), ("'", (4, 5)), ('s', (5, 6)), ('your', (7, 11)), ('nickname', (12, 20)), ('?', (20, 21)), ('My', (22, 24)), ('nickname', (25, 33)), ('is', (34, 36)), ('netkiller', (37, 46)), ('.', (46, 47))]
[('https', (0, 5)), ('://', (5, 8)), ('www', (8, 11)), ('.', (11, 12)), ('netkiller', (12, 21)), ('.', (21, 22)), ('cn', (22, 24))]		
		]]>
			</screen>
		</section>
	</section>
	<section id="transformers">
		<title>transformers</title>
		<section>
			<title>安装 transformers</title>
			<para>安装 transformers</para>
			<programlisting>
	<![CDATA[
pip install transformers	
	]]>
			</programlisting>
		</section>
		<section>
			<title>加载本地模型</title>
			<programlisting>
		<![CDATA[
#!/usr/bin/python
# -*-coding：utf-8-*-
from transformers import BertTokenizer, BertModel

model = "./transformers/bert-base-chinese"
tokenizer = BertTokenizer.from_pretrained(model)
print(tokenizer.tokenize("I have a good time, thank you."))		
		]]>
			</programlisting>
			<screen>
		<![CDATA[
neo@MacBook-Pro-M2 ~/w/test> ll transformers/bert-base-chinese/
total 804576
-rw-r--r--  1 neo  staff   624B 10 23 17:00 config.json
-rw-r--r--  1 neo  staff   392M 10 23 17:05 model.safetensors
-rw-r--r--  1 neo  staff   263K 10 23 17:05 tokenizer.json
-rw-r--r--  1 neo  staff    29B 10 23 17:05 tokenizer_config.json
-rw-r--r--@ 1 neo  staff   107K 10 23 16:13 vocab.txt		
		]]>
			</screen>
			<para>运行结果</para>
			<screen>
		<![CDATA[
neo@MacBook-Pro-M2 ~/w/test> python3 /Users/neo/workspace/test/test.py
['[UNK]', 'have', 'a', 'good', 'time', ',', 'than', '##k', 'you', '.']		
		]]>
			</screen>
		</section>
		<section>
			<title>自动下载模型</title>
			<programlisting>
		<![CDATA[
#!/usr/bin/python
# -*-coding：utf-8-*-
from transformers import AutoTokenizer, AutoModel

modelPath = "hfl/chinese-macbert-base"
tokenizer = AutoTokenizer.from_pretrained(modelPath)
print(tokenizer.tokenize("I have a good time, thank you."))		
		]]>
			</programlisting>

		</section>
		<section>
			<title>编码</title>
			<programlisting>
		<![CDATA[
#!/usr/bin/python
# -*-coding：utf-8-*-
from transformers import BertTokenizer, BertModel

model_path = "./transformers/bert-base-chinese"
tokenizer = BertTokenizer.from_pretrained(model_path)

print(tokenizer.encode('你好世界')) 
print(tokenizer.encode_plus('我不喜欢这世界','你好中国'))
print(tokenizer.convert_ids_to_tokens(tokenizer.encode('我爱你')))		
		]]>
			</programlisting>
		</section>
		<section>
			<title>计算向量</title>
			<programlisting>
		<![CDATA[
from transformers import AutoTokenizer, AutoModel

cache_dir = "/tmp/transformers"
# model = "hfl/chinese-macbert-base"
pretrained_model_name_or_path = "bert-base-chinese"

tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path, cache_dir=cache_dir)  # , force_download=True
model = AutoModel.from_pretrained(pretrained_model_name_or_path, cache_dir=cache_dir)

sentences = ['https://www.netkiller.cn']

inputs = tokenizer(sentences, return_tensors="pt")
outputs = model(**inputs)
array = outputs.pooler_output.tolist()
print(array)		
		]]>
			</programlisting>
		</section>
		<section>
			<title>FAQ</title>
			<section>
				<title>隐藏警告</title>
				<screen>
			<![CDATA[
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).			
			]]>
				</screen>
				<para>解决方案</para>
				<screen>
			<![CDATA[
from transformers import logging
logging.set_verbosity_warning()

或：

from transformers import logging
logging.set_verbosity_error()			
			]]>
				</screen>
				<para>打开 config.json 文件，查看 architectures 配置项，讲 BertModel 更换为
					BertForMaskedLM 即可</para>
				<programlisting>
			<![CDATA[
{
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 21128
}
			]]>
				</programlisting>
				<screen>
			<![CDATA[
Some weights of the model checkpoint at ./transformers/bert-base-chinese were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).			
			]]>
				</screen>
				<para>添加 from_tf=True 参数</para>
				<programlisting>
			<![CDATA[
#!/usr/bin/python
# -*-coding：utf-8-*-
from transformers import BertTokenizer, BertModel,BertConfig,BertForMaskedLM
pretrained_model_name_or_path = "./transformers/bert-base-chinese"
tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path)
model = BertForMaskedLM.from_pretrained(pretrained_model_name_or_path, from_tf=True)
			]]>
				</programlisting>
				<screen>
			<![CDATA[
All TF 2.0 model weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the TF 2.0 model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.			
			]]>
				</screen>
			</section>
		</section>
	</section>
</section>