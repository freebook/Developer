<?xml version="1.0" encoding="UTF-8"?>
<section>
	<title>什么是归一化？</title>
	<para>归一化是数据预处理中的一种常用技术，旨在将数据按比例缩放，使之落入一个小的特定区间，通常是[0, 1]或[-1, 1]。</para>
	<section>
		<title>最小-最大 归一化（Min-Max Scaling）</title>
		<para>计算方法</para>
		<screen>
		<![CDATA[
num = [2.5,3.5,0.5,1.5]

for n in num:
    result = (n-min(num))/(max(num)-min(num))
    print(round(result,4), end=' ')		
		]]>
		</screen>
	</section>
</section>
<section>
	<title>什么是残差</title>
	<para>在数学和统计学领域，残差（Residual）是指观测值与预测值（或拟合值）之间的差异。设观测值为，预测值为，那么残差。例如，我们通过一个线性回归模型来预测房屋价格。对于某一套房子，实际价格（观测值）是 200 万元，而模型预测出来的价格（预测值）是 180 万元，那么残差就是万元。</para>
	<screen>
	<![CDATA[
残差在回归分析中的应用
评估模型拟合优度：残差可以帮助我们了解模型对数据的拟合程度。如果残差较小，说明模型能够较好地拟合数据。例如，在简单线性回归中，我们可以绘制残差图（以预测值为横坐标，残差为纵坐标）。如果残差在零点附近随机分布，没有明显的规律（如系统性的上升或下降趋势、聚集在某个区域等），这表明模型拟合得比较好。相反，如果残差呈现出某种规律性，比如残差随着预测值的增加而增大，那就意味着模型可能存在问题，比如遗漏了重要的变量或者模型的形式不适合数据。
检测异常值：残差较大的观测点可能是异常值。继续以房屋价格预测为例，如果大部分房子的残差都在 - 10 万元到 10 万元之间，但是有一套房子的残差达到了 100 万元，这就需要我们进一步检查这个观测点。它可能是因为数据录入错误，或者这套房子有一些特殊的属性（如独特的地理位置、豪华的装修等）没有被模型考虑到。

残差在时间序列分析中的应用
在时间序列分析中，残差用于衡量模型对时间序列数据的预测准确性。例如，我们用一个 ARIMA 模型（自回归移动平均模型）来预测某产品的销售量。残差可以帮助我们判断模型是否能够很好地捕捉时间序列的波动特征。如果残差序列中存在明显的自相关性（即残差的当前值与过去值之间存在关联），这可能意味着模型没有充分利用数据中的信息，需要进一步调整模型的参数或者结构。

残差在机器学习中的应用
在机器学习模型评估中，残差也是一个重要的概念。以神经网络模型为例，训练集和测试集上的残差大小可以反映模型的泛化能力。如果模型在训练集上残差很小，但在测试集上残差很大，这可能是出现了过拟合现象。也就是说，模型过度学习了训练集中的细节，而不能很好地推广到新的数据上。我们可以通过一些技术，如正则化等，来减少过拟合，使得模型在测试集上的残差也能保持在一个合理的范围内。	
	]]>
	</screen>
</section>

<section>
	<title>网络模型的选择</title>
	<para>随着深度学习的火热，计算机视觉领域内的卷积神经网络模型也层出不穷。</para>
	<para>所以模型不断推陈出新，有些模型已经很老，甚至已经淘汰，不会再被用在生产环境，我们也不必去学习。</para>
	<graphic format="png" fileref="images/ai/timeline.webp"
		width="100%" srccredit="neo" />
	<para>例如做图像识别，建议直接使用 yolo11 不要再研究 VGG,LeNet,AlexNet
		这种古董。新模型建立在新技术基础之上，伴随着性能提升等等</para>
</section>
<section>
	<title>向量数据处理</title>
	<section id="tokenizers">
		<title>tokenizers</title>
		<section>
			<title>Normalization</title>
			<para>文本清洗，Normalization 对原始文本 sentence 执行一系列清洗操作，如：删除空格、去除重音字符、小写化
			</para>
			<programlisting>
		<![CDATA[
from tokenizers import normalizers
from tokenizers.normalizers import NFD, StripAccents

normalizer = normalizers.Sequence([NFD(), StripAccents()])
text = normalizer.normalize_str("Héllò hôw are ü?")
print(text)
# "Hello how are u?"		
		]]>
			</programlisting>
		</section>
		<section>
			<title>Pre-Tokenization</title>
			<para>拆分文本，并标记文本的位置</para>
			<programlisting>
		<![CDATA[
from tokenizers.pre_tokenizers import Whitespace
from tokenizers import pre_tokenizers
from tokenizers.pre_tokenizers import Digits

#Whitespace使用正则表达式\w+|[^\w\s]+，即以word开头，以空格或非word结尾来拆分token，返回数据  List[Tuple[str, Offsets]]:
pre_tokenizer = Whitespace()
data1 = pre_tokenizer.pre_tokenize_str("What's your nickname? My nickname is netkiller.")
print(data1)

pre_tokenizer = pre_tokenizers.Sequence([Whitespace(), Digits(individual_digits=True)])
data2 = pre_tokenizer.pre_tokenize_str("https://www.netkiller.cn")
print(data2)		
		]]>
			</programlisting>
			<para>输出结果</para>
			<screen>
		<![CDATA[
[('What', (0, 4)), ("'", (4, 5)), ('s', (5, 6)), ('your', (7, 11)), ('nickname', (12, 20)), ('?', (20, 21)), ('My', (22, 24)), ('nickname', (25, 33)), ('is', (34, 36)), ('netkiller', (37, 46)), ('.', (46, 47))]
[('https', (0, 5)), ('://', (5, 8)), ('www', (8, 11)), ('.', (11, 12)), ('netkiller', (12, 21)), ('.', (21, 22)), ('cn', (22, 24))]		
		]]>
			</screen>
		</section>
	</section>
	<section id="transformers">
		<title>transformers</title>
		<section>
			<title>安装 transformers</title>
			<para>安装 transformers</para>
			<programlisting>
	<![CDATA[
pip install transformers	
	]]>
			</programlisting>
		</section>
		<section>
			<title>加载本地模型</title>
			<programlisting>
		<![CDATA[
#!/usr/bin/python
# -*-coding：utf-8-*-
from transformers import BertTokenizer, BertModel

model = "./transformers/bert-base-chinese"
tokenizer = BertTokenizer.from_pretrained(model)
print(tokenizer.tokenize("I have a good time, thank you."))		
		]]>
			</programlisting>
			<screen>
		<![CDATA[
neo@MacBook-Pro-M2 ~/w/test> ll transformers/bert-base-chinese/
total 804576
-rw-r--r--  1 neo  staff   624B 10 23 17:00 config.json
-rw-r--r--  1 neo  staff   392M 10 23 17:05 model.safetensors
-rw-r--r--  1 neo  staff   263K 10 23 17:05 tokenizer.json
-rw-r--r--  1 neo  staff    29B 10 23 17:05 tokenizer_config.json
-rw-r--r--@ 1 neo  staff   107K 10 23 16:13 vocab.txt		
		]]>
			</screen>
			<para>运行结果</para>
			<screen>
		<![CDATA[
neo@MacBook-Pro-M2 ~/w/test> python3 /Users/neo/workspace/test/test.py
['[UNK]', 'have', 'a', 'good', 'time', ',', 'than', '##k', 'you', '.']		
		]]>
			</screen>
		</section>
		<section>
			<title>自动下载模型</title>
			<programlisting>
		<![CDATA[
#!/usr/bin/python
# -*-coding：utf-8-*-
from transformers import AutoTokenizer, AutoModel

modelPath = "hfl/chinese-macbert-base"
tokenizer = AutoTokenizer.from_pretrained(modelPath)
print(tokenizer.tokenize("I have a good time, thank you."))		
		]]>
			</programlisting>

		</section>
		<section>
			<title>编码</title>
			<programlisting>
		<![CDATA[
#!/usr/bin/python
# -*-coding：utf-8-*-
from transformers import BertTokenizer, BertModel

model_path = "./transformers/bert-base-chinese"
tokenizer = BertTokenizer.from_pretrained(model_path)

print(tokenizer.encode('你好世界')) 
print(tokenizer.encode_plus('我不喜欢这世界','你好中国'))
print(tokenizer.convert_ids_to_tokens(tokenizer.encode('我爱你')))		
		]]>
			</programlisting>
		</section>
		<section>
			<title>计算向量</title>
			<programlisting>
		<![CDATA[
from transformers import AutoTokenizer, AutoModel

cache_dir = "/tmp/transformers"
# model = "hfl/chinese-macbert-base"
pretrained_model_name_or_path = "bert-base-chinese"

tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path, cache_dir=cache_dir)  # , force_download=True
model = AutoModel.from_pretrained(pretrained_model_name_or_path, cache_dir=cache_dir)

sentences = ['https://www.netkiller.cn']

inputs = tokenizer(sentences, return_tensors="pt")
outputs = model(**inputs)
array = outputs.pooler_output.tolist()
print(array)		
		]]>
			</programlisting>
		</section>
		<section>
			<title>FAQ</title>
			<section>
				<title>隐藏警告</title>
				<screen>
			<![CDATA[
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).			
			]]>
				</screen>
				<para>解决方案</para>
				<screen>
			<![CDATA[
from transformers import logging
logging.set_verbosity_warning()

或：

from transformers import logging
logging.set_verbosity_error()			
			]]>
				</screen>
				<para>打开 config.json 文件，查看 architectures 配置项，讲 BertModel 更换为
					BertForMaskedLM 即可</para>
				<programlisting>
			<![CDATA[
{
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 21128
}
			]]>
				</programlisting>
				<screen>
			<![CDATA[
Some weights of the model checkpoint at ./transformers/bert-base-chinese were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).			
			]]>
				</screen>
				<para>添加 from_tf=True 参数</para>
				<programlisting>
			<![CDATA[
#!/usr/bin/python
# -*-coding：utf-8-*-
from transformers import BertTokenizer, BertModel,BertConfig,BertForMaskedLM
pretrained_model_name_or_path = "./transformers/bert-base-chinese"
tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path)
model = BertForMaskedLM.from_pretrained(pretrained_model_name_or_path, from_tf=True)
			]]>
				</programlisting>
				<screen>
			<![CDATA[
All TF 2.0 model weights were used when initializing BertForMaskedLM.

All the weights of BertForMaskedLM were initialized from the TF 2.0 model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.			
			]]>
				</screen>
			</section>
		</section>
	</section>
</section>